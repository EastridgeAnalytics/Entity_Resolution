{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Resolution Demo Notebook\n",
    "\n",
    "This notebook was created to showcase how Neo4j can help to indentfy and resolve duplication cause by near-similarities in your database. \n",
    "\n",
    "There are a few pre-requisites to take care of before we get started. \n",
    "\n",
    "\n",
    "## 0. Pre-requisites\n",
    "\n",
    "\n",
    "### Python Packages\n",
    "Be sure you have installed the following python packages\n",
    "\n",
    "`pip3 install faker neo4j`\n",
    "\n",
    "or \n",
    "\n",
    "`conda install faker conda-forge::neo4j-python-driver`\n",
    "\n",
    "If conda isntall is not working, please refer to [anaconda.org](https://anaconda.org/conda-forge/neo4j-python-driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "import time\n",
    "import re\n",
    "from faker import Faker\n",
    "from neo4j import GraphDatabase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neo4j DataBase\n",
    "\n",
    "Before starting this demo, make sure to create a new database and update the URI AND PASSWORD DB_NAME below. You may also need to update the USER or DB_NAM if those are different for your DB instance. \n",
    "\n",
    "We recommend using [Neo4j Desktop](https://neo4j.com/product/#neo4j-desktop) as it allows you to use Graph Data Science Library at no cost.  If you are using Neo4J Desktop, you can find your URI by clicking Details and copying the \"Bolt port\".\n",
    "\n",
    "\n",
    "You will also need to ensure APOC and Graph Data Science are enabled on your instance. If you do not do this, you will receive errors later in the notebook.\n",
    "- [Installing APOC](https://neo4j.com/docs/apoc/current/installation/)\n",
    "- [Installing GDC](https://neo4j.com/docs/graph-data-science/current/installation/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neo4j connection details\n",
    "URI = \"bolt://localhost:7687\"\n",
    "USER = \"neo4j\"\n",
    "PASSWORD = \"password\"   \n",
    "DB_NAME = \"neo4j\"   \n",
    "BATCH_ID = \"batch1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of total candidate nodes \n",
    "TOTAL_NODES = 10000\n",
    "\n",
    "# Percentage of near-duplicate s to inject (e.g., 10% means 1000 duplicates)\n",
    "DUPLICATE_PERCENT = 0.1\n",
    "\n",
    "# for keeping track of the program execution time\n",
    "program_start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create mock data candidates\n",
    "\n",
    "  - Inserts 10,000 Candidate nodes using Faker data.\n",
    "  - Intentionally seeds near-duplicates to test entity resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Neo4j.\n",
      "Old Candidate nodes for batch 'batch1' removed.\n",
      "Inserted batch up to index 0\n",
      "Inserted batch up to index 1000\n",
      "Inserted batch up to index 2000\n",
      "Inserted batch up to index 3000\n",
      "Inserted batch up to index 4000\n",
      "Inserted batch up to index 5000\n",
      "Inserted batch up to index 6000\n",
      "Inserted batch up to index 7000\n",
      "Inserted batch up to index 8000\n",
      "Inserted batch up to index 9000\n",
      "Inserted batch up to index 10000\n",
      "Mock data generation complete.\n"
     ]
    }
   ],
   "source": [
    "def create_mock_data(cleanup=True):\n",
    "    \"\"\"Generates random candidate data and inserts into Neo4j. Optionally cleans up old data.\"\"\"\n",
    "    fake = Faker()\n",
    "    Faker.seed(42)  # Ensure some deterministic behavior\n",
    "    \n",
    "    driver = GraphDatabase.driver(URI, auth=(USER, PASSWORD))\n",
    "    print(\"Connected to Neo4j.\")\n",
    "    \n",
    "    # Cleanup old Candidates from previous batch (optional)\n",
    "    if cleanup:\n",
    "        cleanup_previous_batch(driver)\n",
    "    \n",
    "    # Generate random data \n",
    "    candidates = []\n",
    "    \n",
    "    for i in range(TOTAL_NODES):\n",
    "        full_name = fake.name()\n",
    "        email = fake.email()\n",
    "        phone = fake.phone_number()\n",
    "        address = fake.address().replace('\\n', ', ')\n",
    "        \n",
    "        candidate = {\n",
    "            \"candidateId\": f\"cand_{i}\",\n",
    "            \"batchId\": BATCH_ID,\n",
    "            \"fullName\": full_name,\n",
    "            \"email\": email,\n",
    "            \"phoneNumber\": phone,\n",
    "            \"address\": address\n",
    "        }\n",
    "        candidates.append(candidate)\n",
    "    \n",
    "    # Create near duplicates by introducing small variations \n",
    "    num_duplicates = int(TOTAL_NODES * DUPLICATE_PERCENT)\n",
    "    for _ in range(num_duplicates):\n",
    "        original = random.choice(candidates)\n",
    "        # Make a slight variation of the original \n",
    "        alt_full_name = introduce_small_typo(original['fullName'])\n",
    "        alt_email = introduce_small_typo(original['email'])\n",
    "        alt_phone = introduce_phone_variation(original['phoneNumber'])\n",
    "        alt_address = introduce_small_typo(original['address'])\n",
    "        \n",
    "        cand_id = f\"dup_{original['candidateId']}_{random.randint(1,100000)}\"\n",
    "        duplicate = {\n",
    "            \"candidateId\": cand_id,\n",
    "            \"batchId\": BATCH_ID,\n",
    "            \"fullName\": alt_full_name,\n",
    "            \"email\": alt_email,\n",
    "            \"phoneNumber\": alt_phone,\n",
    "            \"address\": alt_address\n",
    "        }\n",
    "        candidates.append(duplicate)\n",
    "    \n",
    "    # Shuffle final list to ensure random distribution \n",
    "    random.shuffle(candidates)\n",
    "    \n",
    "    # Insert into Neo4j in batches\n",
    "    batch_size = 1000\n",
    "    with driver.session(database=DB_NAME) as session:\n",
    "        for i in range(0, len(candidates), batch_size):\n",
    "            batch = candidates[i:i+batch_size]\n",
    "            cypher = \"\"\"\n",
    "            UNWIND $rows AS row\n",
    "            CREATE (c:Candidate {\n",
    "              candidateId: row.candidateId,\n",
    "              batchId: row.batchId,\n",
    "              fullName: row.fullName,\n",
    "              email: row.email,\n",
    "              phoneNumber: row.phoneNumber,\n",
    "              address: row.address\n",
    "            })\n",
    "            \"\"\"\n",
    "            session.run(cypher, parameters={\"rows\": batch})\n",
    "            print(f\"Inserted batch up to index {i}\")\n",
    "    \n",
    "    driver.close()\n",
    "    print(\"Mock data generation complete.\")\n",
    "\n",
    "def cleanup_previous_batch(driver):\n",
    "    \"\"\"Removes all Candidate nodes from the previous batch.\"\"\"\n",
    "    with driver.session(database=DB_NAME) as session:\n",
    "        session.run(f\"MATCH (c:Candidate {{batchId:'{BATCH_ID}'}}) DETACH DELETE c\")\n",
    "    print(f\"Old Candidate nodes for batch '{BATCH_ID}' removed.\")\n",
    "\n",
    "def introduce_small_typo(original_str):\n",
    "    \"\"\"Randomly introduces a small typo or alteration in the given string.\"\"\"\n",
    "    if not original_str:\n",
    "        return original_str\n",
    "    # If string is short, just shuffle or skip\n",
    "    if len(original_str) < 5:\n",
    "        return original_str\n",
    "    \n",
    "    # 50% chance: skip to avoid too many changes\n",
    "    if random.random() < 0.5:\n",
    "        return original_str\n",
    "    \n",
    "    # Insert or remove a character at random position\n",
    "    s_list = list(original_str)\n",
    "    pos = random.randint(0, len(s_list)-1)\n",
    "    \n",
    "    # 50% chance remove a char, 50% chance replace with random letter\n",
    "    if random.random() < 0.5:\n",
    "        del s_list[pos]\n",
    "    else:\n",
    "        s_list[pos] = chr(random.randint(ord('a'), ord('z')))\n",
    "    \n",
    "    return \"\".join(s_list)\n",
    "\n",
    "def introduce_phone_variation(phone_str):\n",
    "    \"\"\"Randomly modifies phone numbers slightly.\"\"\"\n",
    "    # e.g., remove a digit, add a digit, or do nothing\n",
    "    if not phone_str:\n",
    "        return phone_str\n",
    "    if random.random() < 0.5:\n",
    "        return phone_str  # skip messing with half for variety\n",
    "    \n",
    "    only_digits = ''.join(filter(str.isdigit, phone_str))\n",
    "    if len(only_digits) > 6:\n",
    "        # remove last digit or replace it\n",
    "        if random.random() < 0.5:\n",
    "            only_digits = only_digits[:-1]  # remove last\n",
    "        else:\n",
    "            only_digits = only_digits[:-1] + str(random.randint(0,9))\n",
    "    \n",
    "    # randomly add dash or parentheses to create variety\n",
    "    formatted = only_digits\n",
    "    if len(only_digits) > 3:\n",
    "        formatted = f\"({only_digits[:3]}) {only_digits[3:]}\"\n",
    "    return formatted\n",
    "\n",
    "\n",
    "\n",
    "create_mock_data(cleanup=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create \"fraud family\" clusters\n",
    "\n",
    "Purpose:\n",
    "  - Programmatically creates two small demo clusters in Neo4j:\n",
    "    1) A \"fraud family\" of 5 nodes with partial property overlaps\n",
    "    2) 6 variations of one person with minor differences\n",
    "\n",
    "Note: apoc library must be installed in Neo4j https://neo4j.com/docs/apoc/current/installation/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Neo4j.\n",
      "Created Cluster 1 (Fraud Family).\n",
      "Created Cluster 2 (One Person, 6 variations).\n",
      "Demo clusters inserted successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "CLUSTER_1_DATA = [\n",
    "    {\n",
    "        \"candidateId\": \"FRAUD_101\",\n",
    "        \"batchId\": \"batch1\",\n",
    "        \"fullName\": \"Theodore Chadwick\",\n",
    "        \"email\": \"theo.chadwick@gmail.com\",\n",
    "        \"phoneNumber\": \"555-1234\",\n",
    "        \"address\": \"123 Fraud Rd, Chicago\"\n",
    "    },\n",
    "    {\n",
    "        \"candidateId\": \"FRAUD_102\",\n",
    "        \"batchId\": \"batch1\",\n",
    "        \"fullName\": \"Theo Chadwick\",\n",
    "        \"email\": \"theo_chad@gmail.com\",\n",
    "        \"phoneNumber\": \"555-1234\",\n",
    "        \"address\": \"123 Fraud Road, CHI\"\n",
    "    },\n",
    "    {\n",
    "        \"candidateId\": \"FRAUD_103\",\n",
    "        \"batchId\": \"batch1\",\n",
    "        \"fullName\": \"T. Chadwick\",\n",
    "        \"email\": \"t_chad@gmail.com\",\n",
    "        \"phoneNumber\": \"555-1234\",\n",
    "        \"address\": \"123 Fraud Rd, Chicago\"\n",
    "    },\n",
    "    {\n",
    "        \"candidateId\": \"FRAUD_104\",\n",
    "        \"batchId\": \"batch1\",\n",
    "        \"fullName\": \"Ted Chad\",\n",
    "        \"email\": \"tedChad@gmail.com\",\n",
    "        \"phoneNumber\": \"555-9999\",\n",
    "        \"address\": \"123 Fraudd Rd, Chicago\"\n",
    "    },\n",
    "    {\n",
    "        \"candidateId\": \"FRAUD_105\",\n",
    "        \"batchId\": \"batch1\",\n",
    "        \"fullName\": \"Theo C.\",\n",
    "        \"email\": \"theoc12@gmail.com\",\n",
    "        \"phoneNumber\": \"423-502-1234\",\n",
    "        \"address\": \"123 Fraud Rd, Chicago\"\n",
    "    }\n",
    "]\n",
    "\n",
    "CLUSTER_2_DATA = [\n",
    "    {\n",
    "        \"candidateId\": \"PERSON_201\",\n",
    "        \"batchId\": \"batch1\",\n",
    "        \"fullName\": \"Jessica Parsons\",\n",
    "        \"email\": \"jessparsons@gmail.com\",\n",
    "        \"phoneNumber\": \"423-502-1235\",\n",
    "        \"address\": \"99 Demo Ln, Springfield\"\n",
    "    },\n",
    "    {\n",
    "        \"candidateId\": \"PERSON_202\",\n",
    "        \"batchId\": \"batch1\",\n",
    "        \"fullName\": \"Jess Parsons\",\n",
    "        \"email\": \"jessparsons@gmail.com\",\n",
    "        \"phoneNumber\": \"333-4444\",\n",
    "        \"address\": \"99 Demo Lane, Springfield\"\n",
    "    },\n",
    "    {\n",
    "        \"candidateId\": \"PERSON_203\",\n",
    "        \"batchId\": \"batch1\",\n",
    "        \"fullName\": \"Jessica P.\",\n",
    "        \"email\": \"jparsons@gmail.com\",\n",
    "        \"phoneNumber\": \"3334444\",\n",
    "        \"address\": \"99 Demo Ln, Spring Field\"\n",
    "    },\n",
    "    {\n",
    "        \"candidateId\": \"PERSON_204\",\n",
    "        \"batchId\": \"batch1\",\n",
    "        \"fullName\": \"J. Parsons\",\n",
    "        \"email\": \"j.parsons@gmail.com\",\n",
    "        \"phoneNumber\": \"333-4444\",\n",
    "        \"address\": \"99 Demo LN, Springfield\"\n",
    "    },\n",
    "    {\n",
    "        \"candidateId\": \"PERSON_205\",\n",
    "        \"batchId\": \"batch1\",\n",
    "        \"fullName\": \"Jessy Parson\",\n",
    "        \"email\": \"jessy.parson@gmail.com\",\n",
    "        \"phoneNumber\": \"(333)4444\",\n",
    "        \"address\": \"99 Demo Ln, Springfield\"\n",
    "    },\n",
    "    {\n",
    "        \"candidateId\": \"PERSON_206\",\n",
    "        \"batchId\": \"batch1\",\n",
    "        \"fullName\": \"Parsons, Jessica\",\n",
    "        \"email\": \"parsons_jessica@gmail.com\",\n",
    "        \"phoneNumber\": \"333 4444\",\n",
    "        \"address\": \"99 Demo Lane, Springfield\"\n",
    "    }\n",
    "]\n",
    "\n",
    "def create_clusters():\n",
    "    \"\"\"Inserts the two demo clusters into Neo4j.\"\"\"\n",
    "    driver = GraphDatabase.driver(URI, auth=(USER, PASSWORD))\n",
    "    print(\"Connected to Neo4j.\")\n",
    "\n",
    "    with driver.session(database=DB_NAME) as session:\n",
    "        # Insert cluster 1 data using apoc.periodic.iterate\n",
    "        # We'll build a list of records to pass in \n",
    "        session.run(\"\"\"\n",
    "        CALL apoc.periodic.iterate(\n",
    "          'UNWIND $rows AS row RETURN row',\n",
    "          'CREATE (c:Candidate {\n",
    "             candidateId: row.candidateId,\n",
    "             batchId: row.batchId,\n",
    "             fullName: row.fullName,\n",
    "             email: row.email,\n",
    "             phoneNumber: row.phoneNumber,\n",
    "             address: row.address\n",
    "           })',\n",
    "          {batchSize: 5, parallel: false, params: {rows: $clusterData}}\n",
    "        )\n",
    "        \"\"\", parameters={\"clusterData\": CLUSTER_1_DATA})\n",
    "        print(\"Created Cluster 1 (Fraud Family).\")\n",
    "\n",
    "        # Insert cluster 2 data the same way\n",
    "        session.run(\"\"\"\n",
    "        CALL apoc.periodic.iterate(\n",
    "          'UNWIND $rows AS row RETURN row',\n",
    "          'CREATE (c:Candidate {\n",
    "             candidateId: row.candidateId,\n",
    "             batchId: row.batchId,\n",
    "             fullName: row.fullName,\n",
    "             email: row.email,\n",
    "             phoneNumber: row.phoneNumber,\n",
    "             address: row.address\n",
    "           })',\n",
    "          {batchSize: 6, parallel: false, params: {rows: $clusterData}}\n",
    "        )\n",
    "        \"\"\", parameters={\"clusterData\": CLUSTER_2_DATA})\n",
    "        print(\"Created Cluster 2 (One Person, 6 variations).\")\n",
    "\n",
    "    driver.close()\n",
    "    print(\"Demo clusters inserted successfully.\")\n",
    "\n",
    "create_clusters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Normalize the data\n",
    "\n",
    "#Normaization is the process of converting data into a common format that allows for easier comparison and analysis. This is especially important when dealing with text data, where variations in spelling, punctuation, and formatting can make it difficult to match records accurately. In this exercise, we'll implement a few simple normalization functions for phone numbers, email addresses, and street addresses. These functions will remove non-essential characters, convert text to lowercase, and handle common abbreviations to make the data more consistent and easier to work with.\n",
    "\n",
    "This section of code:\n",
    "  - Applies normalization (stripping punctuation, lowercasing, etc.).\n",
    "  - Writes back normalized fields (normalizedPhone, normalizedEmail, etc.).\n",
    "\n",
    "\n",
    "### First, let's create some helper functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_phone(phone_str):\n",
    "    \"\"\"Remove all non-digit characters. Return None if invalid/empty.\"\"\"\n",
    "    if not phone_str:\n",
    "        return None\n",
    "    digits = re.sub(r'[^0-9]', '', phone_str)\n",
    "    return digits if digits else None\n",
    "\n",
    "def normalize_email(email_str):\n",
    "    \"\"\"Lowercase, trim spaces. Return None if invalid/empty.\"\"\"\n",
    "    if not email_str:\n",
    "        return None\n",
    "    email_str = email_str.strip().lower()\n",
    "    # Optionally handle special domain-based rules, but we'll skip here\n",
    "    return email_str\n",
    "\n",
    "def normalize_address(addr_str):\n",
    "    \"\"\"Lowercase, remove punctuation, naive abbreviation handling.\"\"\"\n",
    "    if not addr_str:\n",
    "        return None\n",
    "    addr_str = addr_str.lower()\n",
    "    addr_str = re.sub(r'[.,#]', '', addr_str)\n",
    "    addr_str = addr_str.replace(' street', ' st')\n",
    "    addr_str = addr_str.replace(' avenue', ' ave')\n",
    "    # Additional domain-specific expansions can be added\n",
    "    return addr_str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we can normalize the data for each candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def normalize_properties():\n",
    "    \"\"\" Normalizes phone, email, and address properties for all candidates in the batch.\"\"\"\n",
    "    driver = GraphDatabase.driver(URI, auth=(USER, PASSWORD))\n",
    "    print(\"Connected to Neo4j for normalization.\")\n",
    "    \n",
    "    with driver.session(database=DB_NAME) as session:\n",
    "        # Retrieve all candidate data\n",
    "        fetch_query = f\"\"\"\n",
    "        MATCH (c:Candidate {{batchId:'{BATCH_ID}'}})\n",
    "        RETURN c.candidateId AS candidateId,\n",
    "               c.phoneNumber AS phone,\n",
    "               c.email AS email,\n",
    "               c.address AS address\n",
    "        \"\"\"\n",
    "        result = session.run(fetch_query)\n",
    "        updates = []\n",
    "        for record in result:\n",
    "            cand_id = record[\"candidateId\"]\n",
    "            phone = normalize_phone(record[\"phone\"])\n",
    "            email = normalize_email(record[\"email\"])\n",
    "            address = normalize_address(record[\"address\"])\n",
    "            \n",
    "            updates.append({\n",
    "                \"candidateId\": cand_id,\n",
    "                \"normalizedPhone\": phone,\n",
    "                \"normalizedEmail\": email,\n",
    "                \"normalizedAddress\": address\n",
    "            })\n",
    "        \n",
    "        # Bulk update in batches to avoid large transactions\n",
    "        batch_size = 1000\n",
    "        for i in range(0, len(updates), batch_size):\n",
    "            batch = updates[i:i+batch_size]\n",
    "            update_cypher = \"\"\"\n",
    "            UNWIND $rows AS row\n",
    "            MATCH (c:Candidate {candidateId: row.candidateId})\n",
    "            SET c.normalizedPhone = row.normalizedPhone,\n",
    "                c.normalizedEmail = row.normalizedEmail,\n",
    "                c.normalizedAddress = row.normalizedAddress\n",
    "            \"\"\"\n",
    "            session.run(update_cypher, parameters={\"rows\": batch})\n",
    "            print(f\"Normalized batch up to index {i}\")\n",
    "    \n",
    "    driver.close()\n",
    "    print(\"Property normalization complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Entity Recognition\n",
    "\n",
    "The code blocks below:\n",
    "  - Creates indexes on normalized fields\n",
    "  - (Optionally) sets up blocking keys\n",
    "  - Performs fuzzy matching (Jaro-Winkler, Levenshtein, etc.) on normalized fields\n",
    "  - Aggregates multiple SIMILAR relationships into AGGREGATED_SIMILAR\n",
    "  - Projects the subgraph into GDS for clustering (Leiden)\n",
    "  - Optionally merges or links duplicates\n",
    "\n",
    "\n",
    "We will first definte the helper functions, and then run the entire pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_candidate_indexes(driver):\n",
    "    \"\"\" Create indexes on normalized fields for faster lookup. \"\"\"\n",
    "    index_queries = [\n",
    "        \"CREATE INDEX candidate_candidateId_index IF NOT EXISTS FOR (c:Candidate) ON (c.candidateId)\",\n",
    "        \"CREATE INDEX candidate_phone_index IF NOT EXISTS FOR (c:Candidate) ON (c.normalizedPhone)\",\n",
    "        \"CREATE INDEX candidate_email_index IF NOT EXISTS FOR (c:Candidate) ON (c.normalizedEmail)\",\n",
    "        \"CREATE INDEX candidate_address_index IF NOT EXISTS FOR (c:Candidate) ON (c.normalizedAddress)\"\n",
    "    ]\n",
    "    with driver.session(database=DB_NAME) as session:\n",
    "        for q in index_queries:\n",
    "            session.run(q)\n",
    "    print(\"Candidate indexes on normalized fields created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_soundex_blocking(driver):\n",
    "    \"\"\" Creates a :BlockKey node for soundex(normalizedFullName) and links Candidates for partial blocking.\"\"\"\n",
    "    with driver.session(database=DB_NAME) as session:\n",
    "        # Remove old BlockKeys if desired\n",
    "        session.run(\"MATCH (b:BlockKey) DETACH DELETE b\")\n",
    "        \n",
    "        query = f\"\"\"\n",
    "        CALL apoc.periodic.iterate(\n",
    "          'MATCH (c:Candidate {{batchId:\"{BATCH_ID}\"}}) RETURN c',\n",
    "          'WITH c, apoc.text.soundex(c.normalizedFullName) AS sdx\n",
    "           MERGE (bk:BlockKey {{value: sdx}})\n",
    "           MERGE (c)-[:HAS_BLOCK]->(bk)',\n",
    "          {{batchSize:1000, parallel:false}}\n",
    "        )\n",
    "        \"\"\"\n",
    "        session.run(query)\n",
    "    print(\"Soundex blocking applied.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_similarity_by_name(driver, jaro_threshold=0.05):\n",
    "    \"\"\"\n",
    "    Jaro-Winkler distance is near 0.0 for identical strings and near 1.0 for different strings.\n",
    "    We'll store similarity as (1 - distance), so we want distance < threshold => similarity > (1-threshold).\n",
    "    Using blocking: only compare candidates that share the same blockKey.\n",
    "    \"\"\"\n",
    "    query = f\"\"\"\n",
    "    CALL apoc.periodic.iterate(\n",
    "      \"MATCH (bk:BlockKey)<-[:HAS_BLOCK]-(c:Candidate {{batchId:'{BATCH_ID}'}})\n",
    "       RETURN bk, c\",\n",
    "      \"MATCH (bk)<-[:HAS_BLOCK]-(c2:Candidate {{batchId:'{BATCH_ID}'}})\n",
    "       WHERE id(c) < id(c2)\n",
    "       WITH c, c2, apoc.text.jaroWinklerDistance(c.normalizedFullName, c2.normalizedFullName) AS dist\n",
    "       WHERE dist < {jaro_threshold}\n",
    "       CREATE (c)-[:SIMILAR {{\n",
    "         comparedProperty: 'fullName',\n",
    "         similarity: (1.0 - dist)\n",
    "       }}]->(c2)\",\n",
    "      {{batchSize:200, parallel:false}}\n",
    "    )\n",
    "    \"\"\"\n",
    "    with driver.session(database=DB_NAME) as session:\n",
    "        session.run(query)\n",
    "    print(\"Created SIMILAR relationships by fullName (Jaro-Winkler).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_similarity_by_email(driver, similarity_threshold=0.9):\n",
    "    \"\"\"\n",
    "    Using Levenshtein-based similarity = 1 - (distance / maxLen).\n",
    "    We'll do a simpler approach (no blocking for email) for illustration, or you can also combine with blockKey.\n",
    "    \"\"\"\n",
    "    query = f\"\"\"\n",
    "    CALL apoc.periodic.iterate(\n",
    "      \"MATCH (c:Candidate {{batchId:'{BATCH_ID}'}}) WHERE c.normalizedEmail IS NOT NULL RETURN c\",\n",
    "      \"MATCH (c2:Candidate {{batchId:'{BATCH_ID}'}}) \n",
    "       WHERE c2.normalizedEmail IS NOT NULL AND id(c) < id(c2)\n",
    "       WITH c, c2,\n",
    "         apoc.text.levenshteinDistance(c.normalizedEmail, c2.normalizedEmail) AS dist,\n",
    "         CASE WHEN size(c.normalizedEmail) >= size(c2.normalizedEmail)\n",
    "              THEN size(c.normalizedEmail)\n",
    "              ELSE size(c2.normalizedEmail)\n",
    "         END AS maxLen\n",
    "       WITH c, c2, dist, maxLen, 1.0 - (toFloat(dist)/toFloat(maxLen)) AS sim\n",
    "       WHERE sim >= {similarity_threshold}\n",
    "       CREATE (c)-[:SIMILAR {{\n",
    "         comparedProperty: 'email',\n",
    "         similarity: sim\n",
    "       }}]->(c2)\",\n",
    "      {{batchSize:200, parallel:false}}\n",
    "    )\n",
    "    \"\"\"\n",
    "    with driver.session(database=DB_NAME) as session:\n",
    "        session.run(query)\n",
    "    print(\"Created SIMILAR relationships by email (Levenshtein).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_similarity_by_phone(driver, similarity_threshold=0.9):\n",
    "    \"\"\" Same approach for phone, using normalizedPhone. \"\"\"\n",
    "    query = f\"\"\"\n",
    "    CALL apoc.periodic.iterate(\n",
    "      \"MATCH (c:Candidate {{batchId:'{BATCH_ID}'}}) WHERE c.normalizedPhone IS NOT NULL RETURN c\",\n",
    "      \"MATCH (c2:Candidate {{batchId:'{BATCH_ID}'}}) \n",
    "       WHERE c2.normalizedPhone IS NOT NULL AND id(c) < id(c2)\n",
    "       WITH c, c2,\n",
    "         apoc.text.levenshteinDistance(c.normalizedPhone, c2.normalizedPhone) AS dist,\n",
    "         CASE WHEN size(c.normalizedPhone) >= size(c2.normalizedPhone)\n",
    "              THEN size(c.normalizedPhone)\n",
    "              ELSE size(c2.normalizedPhone)\n",
    "         END AS maxLen\n",
    "       WITH c, c2, dist, maxLen, 1.0 - (toFloat(dist)/toFloat(maxLen)) AS sim\n",
    "       WHERE sim >= {similarity_threshold}\n",
    "       CREATE (c)-[:SIMILAR {{\n",
    "         comparedProperty: 'phoneNumber',\n",
    "         similarity: sim\n",
    "       }}]->(c2)\",\n",
    "      {{batchSize:200, parallel:false}}\n",
    "    )\n",
    "    \"\"\"\n",
    "    with driver.session(database=DB_NAME) as session:\n",
    "        session.run(query)\n",
    "    print(\"Created SIMILAR relationships by phoneNumber (Levenshtein).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_similarity_by_address(driver, jaro_threshold=0.1):\n",
    "    \"\"\" Compare addresses using Jaro-Winkler distance < threshold => similarity > (1-threshold). \"\"\"\n",
    "    query = f\"\"\"\n",
    "    CALL apoc.periodic.iterate(\n",
    "      \"MATCH (c:Candidate {{batchId:'{BATCH_ID}'}}) WHERE c.normalizedAddress IS NOT NULL RETURN c\",\n",
    "      \"MATCH (c2:Candidate {{batchId:'{BATCH_ID}'}}) \n",
    "       WHERE c2.normalizedAddress IS NOT NULL AND id(c) < id(c2)\n",
    "       WITH c, c2, apoc.text.jaroWinklerDistance(c.normalizedAddress, c2.normalizedAddress) AS dist\n",
    "       WHERE dist < {jaro_threshold}\n",
    "       CREATE (c)-[:SIMILAR {{\n",
    "         comparedProperty: 'address',\n",
    "         similarity: (1.0 - dist)\n",
    "       }}]->(c2)\",\n",
    "      {{batchSize:200, parallel:false}}\n",
    "    )\n",
    "    \"\"\"\n",
    "    with driver.session(database=DB_NAME) as session:\n",
    "        session.run(query)\n",
    "    print(\"Created SIMILAR relationships by address (Jaro-Winkler).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_weights_in_neo4j(driver, weights_map):\n",
    "    \"\"\"\n",
    "    Stash normalized weights in a single :WeightConfig node {name:'default'} for easy reference.\n",
    "    Example usage:\n",
    "      raw_weights = {'fullName': 1.0, 'email': 1.5, 'phoneNumber': 1.2, 'address': 0.8}\n",
    "      store_weights_in_neo4j(driver, raw_weights)\n",
    "    \"\"\"\n",
    "    total = sum(weights_map.values())\n",
    "    normalized = {k: (v / total) for k, v in weights_map.items()}\n",
    "\n",
    "    with driver.session(database=DB_NAME) as session:\n",
    "        # Ensure the WeightConfig node exists\n",
    "        session.run(\"MERGE (w:WeightConfig {name:'default'})\")\n",
    "        \n",
    "        # Use APOC to set dynamic property names\n",
    "        for prop, val in normalized.items():\n",
    "            session.run(\"\"\"\n",
    "                MATCH (w:WeightConfig {name:'default'})\n",
    "                CALL apoc.create.setProperty(w, $prop, $val) YIELD node\n",
    "                RETURN node\n",
    "            \"\"\", parameters={'prop': prop, 'val': val})\n",
    "    \n",
    "    print(\"Stored normalized weights in Neo4j WeightConfig node.\")\n",
    "    return normalized\n",
    "\n",
    "\n",
    "def aggregate_similarity_relationships(driver):\n",
    "    \"\"\"\n",
    "    Combine multiple :SIMILAR edges between the same pair into an :AGGREGATED_SIMILAR with a weightedSum, avgSimilarity, etc.\n",
    "    \"\"\"\n",
    "    # Remove old aggregated relationships\n",
    "    cleanup_query = \"MATCH ()-[r:AGGREGATED_SIMILAR]->() DELETE r\"\n",
    "    \n",
    "    # Weighted Summation logic with correct weighted average calculation\n",
    "    aggregation_query = \"\"\"\n",
    "    MATCH (wc:WeightConfig {name:'default'})\n",
    "    WITH wc\n",
    "    MATCH (c1:Candidate)-[r:SIMILAR]->(c2:Candidate)\n",
    "    WHERE id(c1) < id(c2)\n",
    "    WITH wc, c1, c2, collect(r) AS edges\n",
    "    UNWIND edges AS e\n",
    "    WITH wc, c1, c2, e,\n",
    "         CASE e.comparedProperty\n",
    "           WHEN 'fullName' THEN wc.fullName\n",
    "           WHEN 'email' THEN wc.email\n",
    "           WHEN 'phoneNumber' THEN wc.phoneNumber\n",
    "           WHEN 'address' THEN wc.address\n",
    "           ELSE 0.0\n",
    "         END AS propWeight,\n",
    "         e.similarity AS simVal\n",
    "    WITH c1, c2,\n",
    "         collect(simVal * propWeight) AS weightedSims,\n",
    "         collect(propWeight) AS weights,\n",
    "         collect(e.comparedProperty) AS propsUsed\n",
    "    WITH c1, c2,\n",
    "         reduce(total=0.0, val IN weightedSims | total + val) AS weightedSum,\n",
    "         reduce(total=0.0, w IN weights | total + w) AS totalWeight,\n",
    "         size(weightedSims) AS edgeCount,\n",
    "         propsUsed\n",
    "    CREATE (c1)-[:AGGREGATED_SIMILAR {\n",
    "      weightedSum: weightedSum,\n",
    "      avgSimilarity: CASE WHEN totalWeight > 0 THEN (weightedSum / totalWeight) ELSE 0 END,\n",
    "      propertyCount: edgeCount,\n",
    "      propertiesMatched: propsUsed\n",
    "    }]->(c2)\n",
    "    \"\"\"\n",
    "    \n",
    "    with driver.session(database=DB_NAME) as session:\n",
    "        session.run(cleanup_query)\n",
    "        session.run(aggregation_query)\n",
    "    print(\"AGGREGATED_SIMILAR relationships created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_and_cluster(driver, graph_name=\"candidateSimilarityGraph\"):\n",
    "    \"\"\"\n",
    "    Project the graph into GDS using :AGGREGATED_SIMILAR edges, then run Leiden for community detection.\n",
    "    Write cluster info to each node as 'entityId'.\n",
    "    \"\"\"\n",
    "    with driver.session(database=DB_NAME) as session:\n",
    "        # Drop existing projection if any\n",
    "        try:\n",
    "            session.run(f\"CALL gds.graph.drop('{graph_name}', false)\")\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        create_graph_query = f\"\"\"\n",
    "        CALL gds.graph.project(\n",
    "          '{graph_name}',\n",
    "          'Candidate',\n",
    "          {{\n",
    "            AGGREGATED_SIMILAR: {{\n",
    "              type: 'AGGREGATED_SIMILAR',\n",
    "              orientation: 'UNDIRECTED',\n",
    "              properties: {{\n",
    "                weightedSum: {{\n",
    "                  defaultValue: 0.0\n",
    "                }}\n",
    "              }}\n",
    "            }}\n",
    "          }}\n",
    "        )\n",
    "        \"\"\"\n",
    "        session.run(create_graph_query)\n",
    "        print(f\"Projected graph '{graph_name}' into GDS.\")\n",
    "        \n",
    "        leiden_query = f\"\"\"\n",
    "        CALL gds.leiden.write('{graph_name}', {{\n",
    "          writeProperty: 'entityId',\n",
    "          gamma: 100,\n",
    "          maxLevels: 5,\n",
    "          randomSeed: 42\n",
    "        }})\n",
    "        YIELD communityCount, didConverge\n",
    "        \"\"\"\n",
    "        result = session.run(leiden_query).data()\n",
    "        print(\"Leiden clustering result:\", result)\n",
    "        \n",
    "        # Print distribution\n",
    "        dist_query = \"\"\"\n",
    "        MATCH (c:Candidate)\n",
    "        WHERE c.entityId IS NOT NULL\n",
    "        RETURN c.entityId AS entityId, count(*) AS clusterSize\n",
    "        ORDER BY clusterSize DESC\n",
    "        \"\"\"\n",
    "        dist_result = session.run(dist_query).data()\n",
    "        print(\"Cluster distribution:\", dist_result)\n",
    "        \n",
    "        # Optionally drop the in-memory graph\n",
    "        session.run(f\"CALL gds.graph.drop('{graph_name}')\")\n",
    "        print(f\"Clustering done, 'entityId' assigned to each candidate node.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def merge_high_confidence(driver, threshold=2.5):\n",
    "    \"\"\"\n",
    "    Merge nodes if weightedSum >= threshold. \n",
    "    This is destructive, so use caution. \n",
    "    \"\"\"\n",
    "    query = f\"\"\"\n",
    "    CALL apoc.periodic.iterate(\n",
    "      \"MATCH (c1:Candidate)-[r:AGGREGATED_SIMILAR]->(c2:Candidate)\n",
    "       WHERE r.weightedSum >= {threshold}\n",
    "       RETURN c1, c2\",\n",
    "      \"CALL apoc.refactor.mergeNodes([c1, c2], {{\n",
    "         properties:'combine',\n",
    "         mergeRels:true\n",
    "      }}) YIELD node RETURN node\",\n",
    "      {{batchSize:100, parallel:false}}\n",
    "    )\n",
    "    \"\"\"\n",
    "    with driver.session(database=DB_NAME) as session:\n",
    "        session.run(query)\n",
    "    print(f\"Nodes merged where weightedSum >= {threshold}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_high_confidence(driver, threshold=2.5):\n",
    "    \"\"\"\n",
    "    Non-destructive approach: create a :SAME_AS relationship between duplicates.\n",
    "    \"\"\"\n",
    "    query = f\"\"\"\n",
    "    MATCH (c1:Candidate)-[r:AGGREGATED_SIMILAR]->(c2:Candidate)\n",
    "    WHERE r.weightedSum >= {threshold}\n",
    "    MERGE (c1)-[:SAME_AS {{confidence:r.weightedSum}}]->(c2)\n",
    "    \"\"\"\n",
    "    with driver.session(database=DB_NAME) as session:\n",
    "        session.run(query)\n",
    "    print(f\"Linked nodes with :SAME_AS where weightedSum >= {threshold}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Neo4j for Entity Resolution pipeline.\n",
      "Candidate indexes on normalized fields created.\n",
      "Soundex blocking applied.\n",
      "Created SIMILAR relationships by fullName (Jaro-Winkler).\n",
      "Created SIMILAR relationships by email (Levenshtein).\n",
      "Created SIMILAR relationships by phoneNumber (Levenshtein).\n",
      "Created SIMILAR relationships by address (Jaro-Winkler).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The query used a deprecated function: `id`.} {position: line: 5, column: 11, offset: 121} for query: \"\\n    MATCH (wc:WeightConfig {name:'default'})\\n    WITH wc\\n    MATCH (c1:Candidate)-[r:SIMILAR]->(c2:Candidate)\\n    WHERE id(c1) < id(c2)\\n    WITH wc, c1, c2, collect(r) AS edges\\n    UNWIND edges AS e\\n    WITH wc, c1, c2, e,\\n         CASE e.comparedProperty\\n           WHEN 'fullName' THEN wc.fullName\\n           WHEN 'email' THEN wc.email\\n           WHEN 'phoneNumber' THEN wc.phoneNumber\\n           WHEN 'address' THEN wc.address\\n           ELSE 0.0\\n         END AS propWeight,\\n         e.similarity AS simVal\\n    WITH c1, c2,\\n         collect(simVal * propWeight) AS weightedSims,\\n         collect(propWeight) AS weights,\\n         collect(e.comparedProperty) AS propsUsed\\n    WITH c1, c2,\\n         reduce(total=0.0, val IN weightedSims | total + val) AS weightedSum,\\n         reduce(total=0.0, w IN weights | total + w) AS totalWeight,\\n         size(weightedSims) AS edgeCount,\\n         propsUsed\\n    CREATE (c1)-[:AGGREGATED_SIMILAR {\\n      weightedSum: weightedSum,\\n      avgSimilarity: CASE WHEN totalWeight > 0 THEN (weightedSum / totalWeight) ELSE 0 END,\\n      propertyCount: edgeCount,\\n      propertiesMatched: propsUsed\\n    }]->(c2)\\n    \"\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The query used a deprecated function: `id`.} {position: line: 5, column: 20, offset: 130} for query: \"\\n    MATCH (wc:WeightConfig {name:'default'})\\n    WITH wc\\n    MATCH (c1:Candidate)-[r:SIMILAR]->(c2:Candidate)\\n    WHERE id(c1) < id(c2)\\n    WITH wc, c1, c2, collect(r) AS edges\\n    UNWIND edges AS e\\n    WITH wc, c1, c2, e,\\n         CASE e.comparedProperty\\n           WHEN 'fullName' THEN wc.fullName\\n           WHEN 'email' THEN wc.email\\n           WHEN 'phoneNumber' THEN wc.phoneNumber\\n           WHEN 'address' THEN wc.address\\n           ELSE 0.0\\n         END AS propWeight,\\n         e.similarity AS simVal\\n    WITH c1, c2,\\n         collect(simVal * propWeight) AS weightedSims,\\n         collect(propWeight) AS weights,\\n         collect(e.comparedProperty) AS propsUsed\\n    WITH c1, c2,\\n         reduce(total=0.0, val IN weightedSims | total + val) AS weightedSum,\\n         reduce(total=0.0, w IN weights | total + w) AS totalWeight,\\n         size(weightedSims) AS edgeCount,\\n         propsUsed\\n    CREATE (c1)-[:AGGREGATED_SIMILAR {\\n      weightedSum: weightedSum,\\n      avgSimilarity: CASE WHEN totalWeight > 0 THEN (weightedSum / totalWeight) ELSE 0 END,\\n      propertyCount: edgeCount,\\n      propertiesMatched: propsUsed\\n    }]->(c2)\\n    \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored normalized weights in Neo4j WeightConfig node.\n",
      "AGGREGATED_SIMILAR relationships created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The query used a deprecated field from a procedure. ('schema' returned by 'gds.graph.drop' is deprecated.)} {position: line: 1, column: 1, offset: 0} for query: \"CALL gds.graph.drop('candidateSimilarityGraph', false)\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projected graph 'candidateSimilarityGraph' into GDS.\n"
     ]
    },
    {
     "ename": "ClientError",
     "evalue": "{code: Neo.ClientError.Procedure.ProcedureCallFailed} {message: Failed to invoke procedure `gds.leiden.write`: Caused by: java.lang.NullPointerException: Cannot invoke \"org.neo4j.gds.leiden.LeidenDendrogramManager.getCurrent()\" because the return value of \"org.neo4j.gds.leiden.LeidenResult.dendrogramManager()\" is null}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 43\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEntity Resolution pipeline complete.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     42\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 43\u001b[0m \u001b[43mrun_entity_resolution\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     45\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time  \u001b[38;5;66;03m# seconds\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[22], line 31\u001b[0m, in \u001b[0;36mrun_entity_resolution\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m aggregate_similarity_relationships(driver)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# 5. Project into GDS & cluster\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m \u001b[43mproject_and_cluster\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcandidateSimilarityGraph\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# 6. Merge or link duplicates with high confidence\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Adjust threshold if your total possible weightedSum is 4.5, for instance\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# merge_high_confidence(driver, threshold=2.5)\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# link_high_confidence(driver, threshold=2.5)  # or do merges if desired\u001b[39;00m\n\u001b[0;32m     38\u001b[0m driver\u001b[38;5;241m.\u001b[39mclose()\n",
      "Cell \u001b[1;32mIn[19], line 42\u001b[0m, in \u001b[0;36mproject_and_cluster\u001b[1;34m(driver, graph_name)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProjected graph \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgraph_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m into GDS.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     33\u001b[0m leiden_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;124mCALL gds.leiden.write(\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgraph_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;124m  writeProperty: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentityId\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;124mYIELD communityCount, didConverge\u001b[39m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m---> 42\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleiden_query\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLeiden clustering result:\u001b[39m\u001b[38;5;124m\"\u001b[39m, result)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Print distribution\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ljman\\Documents\\Eastridge\\Entity_Resolution\\.conda\\Lib\\site-packages\\neo4j\\_sync\\work\\result.py:782\u001b[0m, in \u001b[0;36mResult.data\u001b[1;34m(self, *keys)\u001b[0m\n\u001b[0;32m    754\u001b[0m \u001b[38;5;129m@NonConcurrentMethodChecker\u001b[39m\u001b[38;5;241m.\u001b[39m_non_concurrent_method\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdata\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mkeys: _TResultKey) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, t\u001b[38;5;241m.\u001b[39mAny]]:\n\u001b[0;32m    756\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    757\u001b[0m \u001b[38;5;124;03m    Return the remainder of the result as a list of dictionaries.\u001b[39;00m\n\u001b[0;32m    758\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    780\u001b[0m \u001b[38;5;124;03m        Can raise :exc:`.ResultConsumedError`.\u001b[39;00m\n\u001b[0;32m    781\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 782\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[43mrecord\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ljman\\Documents\\Eastridge\\Entity_Resolution\\.conda\\Lib\\site-packages\\neo4j\\_sync\\work\\result.py:782\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    754\u001b[0m \u001b[38;5;129m@NonConcurrentMethodChecker\u001b[39m\u001b[38;5;241m.\u001b[39m_non_concurrent_method\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdata\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mkeys: _TResultKey) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, t\u001b[38;5;241m.\u001b[39mAny]]:\n\u001b[0;32m    756\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    757\u001b[0m \u001b[38;5;124;03m    Return the remainder of the result as a list of dictionaries.\u001b[39;00m\n\u001b[0;32m    758\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    780\u001b[0m \u001b[38;5;124;03m        Can raise :exc:`.ResultConsumedError`.\u001b[39;00m\n\u001b[0;32m    781\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 782\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[43mrecord\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ljman\\Documents\\Eastridge\\Entity_Resolution\\.conda\\Lib\\site-packages\\neo4j\\_sync\\work\\result.py:398\u001b[0m, in \u001b[0;36mResult.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    396\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_record_buffer\u001b[38;5;241m.\u001b[39mpopleft()\n\u001b[0;32m    397\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_streaming:\n\u001b[1;32m--> 398\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_discarding:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_discard()\n",
      "File \u001b[1;32mc:\\Users\\ljman\\Documents\\Eastridge\\Entity_Resolution\\.conda\\Lib\\site-packages\\neo4j\\_sync\\io\\_common.py:184\u001b[0m, in \u001b[0;36mConnectionErrorHandler.__getattr__.<locals>.outer.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 184\u001b[0m         \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (Neo4jError, ServiceUnavailable, SessionExpired) \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    186\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39miscoroutinefunction(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__on_error)\n",
      "File \u001b[1;32mc:\\Users\\ljman\\Documents\\Eastridge\\Entity_Resolution\\.conda\\Lib\\site-packages\\neo4j\\_sync\\io\\_bolt.py:864\u001b[0m, in \u001b[0;36mBolt.fetch_message\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    860\u001b[0m \u001b[38;5;66;03m# Receive exactly one message\u001b[39;00m\n\u001b[0;32m    861\u001b[0m tag, fields \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minbox\u001b[38;5;241m.\u001b[39mpop(\n\u001b[0;32m    862\u001b[0m     hydration_hooks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponses[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mhydration_hooks\n\u001b[0;32m    863\u001b[0m )\n\u001b[1;32m--> 864\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midle_since \u001b[38;5;241m=\u001b[39m monotonic()\n\u001b[0;32m    866\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[1;32mc:\\Users\\ljman\\Documents\\Eastridge\\Entity_Resolution\\.conda\\Lib\\site-packages\\neo4j\\_sync\\io\\_bolt5.py:500\u001b[0m, in \u001b[0;36mBolt5x0._process_message\u001b[1;34m(self, tag, fields)\u001b[0m\n\u001b[0;32m    498\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_server_state_manager\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbolt_states\u001b[38;5;241m.\u001b[39mFAILED\n\u001b[0;32m    499\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 500\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_failure\u001b[49m\u001b[43m(\u001b[49m\u001b[43msummary_metadata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ServiceUnavailable, DatabaseUnavailable):\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool:\n",
      "File \u001b[1;32mc:\\Users\\ljman\\Documents\\Eastridge\\Entity_Resolution\\.conda\\Lib\\site-packages\\neo4j\\_sync\\io\\_common.py:254\u001b[0m, in \u001b[0;36mResponse.on_failure\u001b[1;34m(self, metadata)\u001b[0m\n\u001b[0;32m    252\u001b[0m handler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandlers\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_summary\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    253\u001b[0m Util\u001b[38;5;241m.\u001b[39mcallback(handler)\n\u001b[1;32m--> 254\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hydrate_error(metadata)\n",
      "\u001b[1;31mClientError\u001b[0m: {code: Neo.ClientError.Procedure.ProcedureCallFailed} {message: Failed to invoke procedure `gds.leiden.write`: Caused by: java.lang.NullPointerException: Cannot invoke \"org.neo4j.gds.leiden.LeidenDendrogramManager.getCurrent()\" because the return value of \"org.neo4j.gds.leiden.LeidenResult.dendrogramManager()\" is null}"
     ]
    }
   ],
   "source": [
    "\n",
    "def run_entity_resolution():\n",
    "    driver = GraphDatabase.driver(URI, auth=(USER, PASSWORD))\n",
    "    print(\"Connected to Neo4j for Entity Resolution pipeline.\")\n",
    "    \n",
    "    # 1. Create indexes\n",
    "    create_candidate_indexes(driver)\n",
    "    \n",
    "    # 2. (Optional) Create blocking keys\n",
    "    create_soundex_blocking(driver)\n",
    "    \n",
    "    # 3. Generate :SIMILAR relationships\n",
    "    create_similarity_by_name(driver, jaro_threshold=0.05)\n",
    "    create_similarity_by_email(driver, similarity_threshold=0.9)\n",
    "    create_similarity_by_phone(driver, similarity_threshold=0.9)\n",
    "    create_similarity_by_address(driver, jaro_threshold=0.1)\n",
    "    \n",
    "    # Wait for relationship creation to finish\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # 4. Store & use weights to create :AGGREGATED_SIMILAR\n",
    "    raw_weights = {\n",
    "      \"fullName\": 1.0,\n",
    "      \"email\": 1.5,\n",
    "      \"phoneNumber\": 1.2,\n",
    "      \"address\": 0.8\n",
    "    }\n",
    "    store_weights_in_neo4j(driver, raw_weights)\n",
    "    aggregate_similarity_relationships(driver)\n",
    "    \n",
    "    # 5. Project into GDS & cluster\n",
    "    project_and_cluster(driver, \"candidateSimilarityGraph\")\n",
    "    \n",
    "    # 6. Merge or link duplicates with high confidence\n",
    "    # Adjust threshold if your total possible weightedSum is 4.5, for instance\n",
    "    # merge_high_confidence(driver, threshold=2.5)\n",
    "    # link_high_confidence(driver, threshold=2.5)  # or do merges if desired\n",
    "    \n",
    "    driver.close()\n",
    "    print(\"Entity Resolution pipeline complete.\")\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "run_entity_resolution()\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time  # seconds\n",
    "elapsed_minutes = elapsed_time / 60   # convert seconds to minutes\n",
    "\n",
    "print(f\"Time elapsed: {elapsed_minutes:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Resolution\n",
    "\n",
    "The bloks of code below: \n",
    "- Creates MasterEntity nodes and link each Candidate to its corresponding MasterEntity node using the 'entityId' property.\n",
    "- Links each Candidate node to the corresponding MasterEntity node in batches.\n",
    "- Sets cononical values for each MasterEntity node based on the most common values in the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_master_nodes_and_links():\n",
    "    \"\"\"\n",
    "    1) Creates one distinct MasterEntity node per unique entityId value from the Leiden output\n",
    "       (or whichever community detection algorithm you used to assign 'entityId').\n",
    "    2) Links each Candidate node to the corresponding MasterEntity node in batches.\n",
    "\n",
    "    \"\"\"\n",
    "    driver = GraphDatabase.driver(URI, auth=(USER, PASSWORD))\n",
    "\n",
    "\n",
    "    with driver.session(database=DB_NAME) as session:\n",
    "        # STEP 1: Create distinct MasterEntity nodes\n",
    "        # For each unique entityId, we make a single MasterEntity node\n",
    "        create_master_nodes_query = \"\"\"\n",
    "        CALL apoc.periodic.iterate(\n",
    "          \"MATCH (c:Candidate) \n",
    "           WHERE c.entityId IS NOT NULL\n",
    "           RETURN DISTINCT c.entityId AS communityId\",\n",
    "          \"MERGE (m:MasterEntity {communityId: communityId})\",\n",
    "          {batchSize: 1000, parallel: false}\n",
    "        )\n",
    "        \"\"\"\n",
    "        session.run(create_master_nodes_query)\n",
    "        print(\"Distinct MasterEntity nodes created for each communityId.\")\n",
    "\n",
    "        # STEP 2: Link each Candidate to its MasterEntity node\n",
    "        # For each candidate, MERGE a BELONGS_TO relationship to the matching MasterEntity node\n",
    "        link_candidates_query = \"\"\"\n",
    "        CALL apoc.periodic.iterate(\n",
    "          \"MATCH (c:Candidate) \n",
    "           WHERE c.entityId IS NOT NULL\n",
    "           RETURN c\",\n",
    "          \"MATCH (m:MasterEntity {communityId: c.entityId})\n",
    "           MERGE (c)-[:BELONGS_TO]->(m)\",\n",
    "          {batchSize: 1000, parallel: false}\n",
    "        )\n",
    "        \"\"\"\n",
    "        session.run(link_candidates_query)\n",
    "        print(\"Linked each Candidate node to its corresponding MasterEntity node.\")\n",
    "\n",
    "create_master_nodes_and_links()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_canonical():\n",
    "    \"\"\"\n",
    "    For each MasterEntity node, aggregates the properties of related Candidate nodes (where Candidate.entityId = MasterEntity.communityId),\n",
    "    computes frequency maps for each property using apoc.coll.frequenciesAsMap, and updates the MasterEntity node with canonical values.\n",
    "    The canonical value is taken as the first key from each frequency map.\n",
    "    \"\"\"\n",
    "    driver = GraphDatabase.driver(URI, auth=(USER, PASSWORD))\n",
    "\n",
    "    with driver.session(database=DB_NAME) as session:\n",
    "        query = \"\"\"\n",
    "        CALL apoc.periodic.iterate(\n",
    "          'MATCH (m:MasterEntity) RETURN m',\n",
    "          'MATCH (c:Candidate {entityId: m.communityId})\n",
    "           WITH m, \n",
    "                collect(c.fullName) AS allNames, \n",
    "                collect(c.email) AS allEmails, \n",
    "                collect(c.phoneNumber) AS allPhones, \n",
    "                collect(c.address) AS allAddresses\n",
    "           WITH m,\n",
    "                apoc.coll.frequenciesAsMap(allNames) AS nameFreq,\n",
    "                apoc.coll.frequenciesAsMap(allEmails) AS emailFreq,\n",
    "                apoc.coll.frequenciesAsMap(allPhones) AS phoneFreq,\n",
    "                apoc.coll.frequenciesAsMap(allAddresses) AS addrFreq\n",
    "           WITH m,\n",
    "                keys(nameFreq)[0] AS bestName,\n",
    "                keys(emailFreq)[0] AS bestEmail,\n",
    "                keys(phoneFreq)[0] AS bestPhone,\n",
    "                keys(addrFreq)[0] AS bestAddress\n",
    "           SET m.fullNameCanonical = bestName,\n",
    "               m.emailCanonical = bestEmail,\n",
    "               m.phoneNumberCanonical = bestPhone,\n",
    "               m.addressCanonical = bestAddress',\n",
    "          {batchSize:50, parallel:false}\n",
    "        )\n",
    "        \"\"\"\n",
    "        session.run(query)\n",
    "        print(\"Canonical values updated for MasterEntity nodes.\")\n",
    "\n",
    "set_canonical()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do run all, the block of code below will print out the run time for the entire process!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "program_end_time = time.time()\n",
    "program_elapsed_time = program_end_time - program_start_time  # seconds\n",
    "program_elapsed_minutes = program_elapsed_time / 60   # convert seconds to minutes\n",
    "\n",
    "print(f\"Time elapsed: {program_elapsed_minutes:.2f} minutes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
